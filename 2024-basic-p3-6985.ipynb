{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb622924",
   "metadata": {
    "papermill": {
     "duration": 0.007966,
     "end_time": "2024-08-22T08:43:56.037479",
     "exception": false,
     "start_time": "2024-08-22T08:43:56.029513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Feature Extract**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad70f222",
   "metadata": {
    "papermill": {
     "duration": 0.007022,
     "end_time": "2024-08-22T08:43:56.051811",
     "exception": false,
     "start_time": "2024-08-22T08:43:56.044789",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "두 도메인의 feature를 추출하기에 앞서, 아날로그 형태의 음악 데이터를 학습에 사용하기 위해서는 먼저 windowing(=sampling) 과정을 거쳐주어야 합니다. 또 표준 샘플링 레이트는 44100Hz를 주로 사용합니다. 즉, 1초를 44100등분하여 음악값들을 샘플링 해오겠다는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0137b47",
   "metadata": {
    "papermill": {
     "duration": 0.006936,
     "end_time": "2024-08-22T08:43:56.066037",
     "exception": false,
     "start_time": "2024-08-22T08:43:56.059101",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### **[방법 1] Rhythm domain feature extraction**\n",
    "\n",
    "여러가지 rhythm 도메인의 feature 중 Tempogram을 추출합니다.\n",
    "\n",
    "![image](pic/several_data_p3_4.png)\n",
    "\n",
    "rhythm feature 추출 흐름도는 위와 같습니다.\n",
    "\n",
    "리듬이란, 일반적으로 음악의 기본이 되는 규칙적인 펄스 또는 음악에서 전반적으로 유지되는 반복적 패턴을 의미합니다. 이 때 일정한 간격으로 발생하는 펄스를 비트라고 하며, 비트의 강약이 변화하여 만들어지는 패턴을 박자라고 칭합니다.\n",
    "\n",
    "리듬과 그에 대한 박자는 사실 다양한 변수들이 혼합된 개념이기에 정확히 한 문장으로 정의하기는 힘들지만, 보통 음악적 리듬의 박자 구조는 일정한 간격을 지닌 시간 축 격자에 표시할 수 있는 연속적인 리듬 이벤트를 의미합니다.\n",
    "\n",
    "음악은 박자 시스템에 기반한 리듬 구조를 가지고 있기 때문에 박자 구조를 분석하는 것이 리듬 분석을 위해서는 필수적이라고 볼 수 있습니다. 그리고 이러한 음악의 리듬 패턴을 분석하는데 사용되는 시간적 feature 중 하나가 Tempogram입니다.\n",
    "\n",
    "그렇다면 음악의 Tempogram은 어떻게 추출할 수 있을까요?\n",
    "\n",
    "음악의 Tempogram을 추출하기 위해서는 먼저 windowing을 거쳐 시간별로 쪼개진 데이터를 librosa.onset.onset_strength() 함수의 입력으로 주어 onset 지점을 탐지해야 합니다. Onset이란, 주로 박자의 첫 번째 비트나 강조되는 리듬 변화가 발생하는 시점을 의미합니다. 그렇기 때문에 onset 지점을 검출하는 것이 곧 음악의 리듬을 분석하는 것이고, 이를 수행하는 다양한 방식이 있지만 대표적으로 주파수 스펙트럼의 변화, 에너지 변화 등을 파악하여 onset 지점을 검출합니다.\n",
    "\n",
    "다음으로는 이렇게 탐지된 onset 지점을 작은 프레임 단위로 나누고 각 프레임에 대해 autocorrelation(자기상관) 값을 계산합니다. 이는 각 onset 지점의 자기상관성을 분석한 그래프로, 시간 지연된 타 onset 지점과의 유사도를 계산하여 주기성과 리듬 정보를 파악할 수 있게 됩니다.\n",
    "\n",
    "![image](pic/several_data_p3_5.png)\n",
    "\n",
    "위 그림에서 가운데 그래프가 onset 검출 점수 그래프에 해당합니다. 앞선 과정을 거쳐 시간 지연 축에 대한 자기상관함수 값을 얻었다면, 위 값들을 통해 Tempogram 값을 얻어야 합니다. 현재 자기상관함수는 시간 지연 축에 대해 나타나 있기 때문에, 템포에 대한 축으로 변경해주어야 이 값들을 유의미하게 활용할 수 있습니다.\n",
    "이러한 변형은 아래 두 과정을 거쳐 이루어집니다.\n",
    "\n",
    "- Tempo (in BPM) = 60 / Timelag (in sec) 식을 적용하여 x축 변환\n",
    "- 음악적으로 유의미한 tempo 축을 갖도록 보간 적용\n",
    "\n",
    "![image](pic/several_data_p3_6.png)\n",
    "\n",
    "우선 첫 번째 식을 적용해 자기상관함수의 time-lag 축을 tempo(BPM)축으로 바꾸어줍니다. 이렇게 되면 수식에 따라 tempo 축의 범위가 선형적이지 않은데, 이를 펴주기 위해 tempo 축에 대한 보간을 수행합니다.\n",
    "\n",
    "위와 같은 과정을 거쳐 음악의 Tempogram을 얻을 수 있습니다. 과정이 어렵고 생소할 수 있지만 librosa 라이브러리에서는 단순히 앞서 얻은 onset_strength 값을 librosa.feature.tempogram()의 인자로 넘겨줌으로써 음악의 Tempogram을 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e658c86",
   "metadata": {
    "papermill": {
     "duration": 0.007333,
     "end_time": "2024-08-22T08:43:56.080549",
     "exception": false,
     "start_time": "2024-08-22T08:43:56.073216",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### **[방법 2] Spectral domain feature extraction**\n",
    "spectral 도메인의 여러 feature 중 Chromagram과 MFCC를 추출합니다.\n",
    "\n",
    "![image](pic/several_data_p3_7.png)\n",
    "\n",
    "spectral feature 추출 흐름도는 위와 같습니다.\n",
    "\n",
    "마찬가지로 처음엔 windowing을 거쳐 음성 데이터를 이산화합니다.\n",
    "\n",
    "Windowing 과정을 거쳐 잘게 나누어진 프레임들에 각각 STFT(Short Time Fourier Transform)를 적용해 주파수 도메인으로 변환된 spectrogram을 얻는 과정을 거칩니다. 이에 대해서는 아래에서 계속 설명드리겠습니다.\n",
    "\n",
    "![image](pic/several_data_p3_8.png)\n",
    "\n",
    "STFT(librosa.stft())는 음성 신호의 시간적 정보를 잘 활용하기 위해 각 프레임마다 FFT(Fast Fourier Transform)을 적용하여 시간 도메인의 데이터를 주파수 도메인으로 변환해주는 과정입니다. 일정 시간 영역에는 수많은 주파수가 혼합되어 있어 정보를 분석하기 어렵기 때문에 음성의 고유 배음구조를 유추할 수 있도록 신호를 위 그림과 같이 주파수 영역으로 변환시켜주는 것입니다.\n",
    "\n",
    "![image](pic/several_data_p3_9.png)\n",
    "\n",
    "FFT를 음성의 전체 시간 영역에 한 번에 적용하게 되면 음성의 시계열적 특성 정보를 제대로 활용할 수 없게 됩니다. 따라서 전체 시간 영역에 대해 FFT를 한 번에 적용하는 것이 아닌, 위 그림과 같이 잘게 나뉜 영역에 대해 여러 번(총 window 개수만큼) FFT를 적용하여 주파수 도메인으로 변환(=STFT)해준다면 주파수 정보 뿐만 아니라 시간적 순서와 관련된 time frame에 대한 정보까지 얻을 수 있게 됩니다.\n",
    "\n",
    "STFT를 적용하여 얻은 값(=spectrum)의 magnitude를 사용하기 위해 절대값을 취하여 spectrogram을 얻고, 이렇게 얻은 spectrogram을 제곱하여 power spectrogram을 얻습니다. 다음 함수의 입력으로 spectrogram이 아니라 power spectrogram이 사용되는 이유는 주파수 도메인에서 시간에 따른 신호의 에너지를 강조해줄 수 있기 때문입니다. 제곱을 함으로써 음악 신호의 패턴을 더 쉽게 감지하고, 음악에서 특징점이 될 수 있는 부분을 잡아내는 데에 더욱 도움되는 값을 얻어낼 수 있는 것입니다.\n",
    "\n",
    "##### **1) Chromagram**\n",
    "\n",
    "Chromagram이란 음악의 주파수 정보를 사용해 음악의 음계 기반 특성을 파악하는 방법입니다. 음악의 주파수 영역에서 각 음높이에 해당하는 성분들의 빈도 정보를 표현하여 음악의 화음과 멜로디 정보를 추출할 수 있게 되는 것입니다.\n",
    "\n",
    "Chromagram은 기본값으로 하나의 프레임을 12차원의 벡터로 표현하는데, 여기서 12란 숫자는 서양 음악에서 사용되는 12음계 체계(도, 도#, 레, 레#, 미, 파, 파#, 솔, 솔#, 라, 라#, 시)를 기반으로 설정된 것입니다. 참고로 이 12음계를 chromatic scale이라 부르고, 이 음계를 기반으로 분석하는 것이 Chromagram인 것입니다.\n",
    "\n",
    "![image](pic/several_data_p3_10.png)\n",
    "\n",
    "앞서 얻은 power spectrogram을 입력으로 주어 각 프레임에서 12개의 음들이 얼마나 등장하는지를 나타내는 크로마 밴드를 구하고, 이에 해당하는 파형 크기의 제곱 값을 합하여 Chromagram을 만들어냅니다. 위 그림은 12개의 음계를 바탕으로 만들어낸 음악의 Chromagram입니다.\n",
    "\n",
    "Chromagram은 librosa.feature.chroma_stft()에 power spectrogram을 입력으로 주어 추출할 수 있습니다.\n",
    "\n",
    "##### **2) MFCC**\n",
    "\n",
    "MFCC는 Mel Frequency Cepstral Coefficient의 약자입니다. 다양한 음성 feature가 존재하지만, MFCC는 사람의 청각 기관 특성을 가장 잘 살려 설계된 feature 중 하나에 해당하고 현재까지도 음성, 음악 관련 task를 수행하기 위해 많이 사용됩니다.\n",
    "\n",
    "다시 전체 파이프라인 그림으로 올라가보시면 MFCC는 power spectrogram에 melfilter 필터를 적용해 melspectrogram(librosa.feature.melspectrogram())을 추출하고, 켑스트럴 분석(DCT, Discrete Cosine Transform, librosa.power_to_db())를 거쳐 얻게됩니다.\n",
    "\n",
    "**Melspectrogram**<br>\n",
    "\n",
    "실제로 사람의 달팽이관은 저주파 대역을 감지하는 부분은 굵고, 고주파 대역을 감지하는 부분은 얇습니다. Melfilter는 이렇게 사람의 청각기관이 높은 주파수보다 낮은 주파수 대역에 더욱 민감하게 반응한다는 사실을 반영하여 power spectrogram을 조정하는 역할을 수행합니다.\n",
    "\n",
    "![image](pic/several_data_p3_11.png)\n",
    "\n",
    "위 그림은 melfilter인데 상대적으로 높은 주파수 대역으로 갈수록 필터가 감소하는 것을 볼 수 있습니다. 이를 적용해 사람이 더욱 민감하게 반응하는 주파수 대역을 강조한 값으로 변환된 melspectrogram을 얻을 수 있게 됩니다.\n",
    "\n",
    "**Cepstral analysis**<br>\n",
    "\n",
    "다음은 앞서 얻은 melspectrogram에 켑스트럴 분석 과정을 적용하고 이를 통해 최종적인 MFCC feature를 얻을 수 있습니다.\n",
    "과정상으로 보면, melspectrogram을 log scale로 만들어 log spectrogram을 얻고 이에 DCT(Discrete Cosine Transform)를 적용하여 최종 MFCC feature를 추출하게 되는 것입니다.\n",
    "\n",
    "여기서 log scale로 만드는 이유는, 사람의 소리 인식이 log scale에 가깝기 때문입니다. 다시 말해 사람이 두 배 큰 소리라고 인식하기 위해서는, 실제로 에너지가 100배 큰 소리여야 한다는 특성을 feature 추출 과정에 적용해주었다는 뜻입니다. 다음으로 DCT는 대부분의 실제 신호가 우함수이기 때문에 입력신호를 우함수인 코사인 함수의 합으로 표현해주는 과정입니다. DCT의 변환 결과는 계수 형태로 나타나며, log spectrogram에 DCT를 적용했을 때 주파수 성분으로 분해하여 얻은 계수 중 주로 사용되는 일부 계수들만 선택하면 그것이 MFCC feature로서 사용되는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e9b306c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T08:43:56.096179Z",
     "iopub.status.busy": "2024-08-22T08:43:56.095863Z",
     "iopub.status.idle": "2024-08-22T08:44:02.540967Z",
     "shell.execute_reply": "2024-08-22T08:44:02.540213Z"
    },
    "papermill": {
     "duration": 6.455558,
     "end_time": "2024-08-22T08:44:02.543235",
     "exception": false,
     "start_time": "2024-08-22T08:43:56.087677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'librosa'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'librosa'"
     ]
    }
   ],
   "source": [
    "# 라이브러리 임포트\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d182cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T08:44:02.559837Z",
     "iopub.status.busy": "2024-08-22T08:44:02.559248Z",
     "iopub.status.idle": "2024-08-22T08:44:02.564442Z",
     "shell.execute_reply": "2024-08-22T08:44:02.563707Z"
    },
    "papermill": {
     "duration": 0.015333,
     "end_time": "2024-08-22T08:44:02.566223",
     "exception": false,
     "start_time": "2024-08-22T08:44:02.550890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 하이퍼파라미터\n",
    "args = {\n",
    "    \"train_path\" : \"/kaggle/input/2024-outta-basic-p-3/train\",\n",
    "    \"test_path\" : \"/kaggle/input/2024-outta-basic-p-3/test\",\n",
    "    \"submit_path\" : \"/kaggle/input/2024-outta-basic-p-3/sample_submission.csv\",\n",
    "    \"extract_features\" : \"spectral\",  # \"rhythm\"과 \"spectral\" 중에 선택하세요.\n",
    "    \"batch_size\" : 32,\n",
    "    \"num_labels\" : 2,\n",
    "    \"epochs\" : 10,\n",
    "    \"lr\" : 2e-5, \n",
    "    \"eps\" : 1e-8,\n",
    "    \"seed_val\" : 42     # 절대 수정하지 마세요.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c103f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T08:44:02.581896Z",
     "iopub.status.busy": "2024-08-22T08:44:02.581629Z",
     "iopub.status.idle": "2024-08-22T08:44:02.672865Z",
     "shell.execute_reply": "2024-08-22T08:44:02.671890Z"
    },
    "papermill": {
     "duration": 0.101711,
     "end_time": "2024-08-22T08:44:02.675262",
     "exception": false,
     "start_time": "2024-08-22T08:44:02.573551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 랜덤시드 고정하기\n",
    "seed = args[\"seed_val\"]\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available() : \n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# 디바이스 선택\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c950ec92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T08:44:02.691735Z",
     "iopub.status.busy": "2024-08-22T08:44:02.691423Z",
     "iopub.status.idle": "2024-08-22T08:44:15.956028Z",
     "shell.execute_reply": "2024-08-22T08:44:15.954799Z"
    },
    "papermill": {
     "duration": 13.275115,
     "end_time": "2024-08-22T08:44:15.958168",
     "exception": false,
     "start_time": "2024-08-22T08:44:02.683053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\r\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\r\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\r\n",
      "Installing collected packages: torchsummary\r\n",
      "Successfully installed torchsummary-1.5.1\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchsummary\n",
    "from torchsummary import summary as summary_## 모델 정보를 확인하기 위해 torchsummary 함수 import\n",
    "\n",
    "## 모델의 형태를 출력하기 위한 함수 \n",
    "def summary_model(model, input_shape=(384,)):\n",
    "    model = model.to(device)\n",
    "    summary_(model, input_shape) ## (model, (input shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4287049",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T08:44:15.976252Z",
     "iopub.status.busy": "2024-08-22T08:44:15.975913Z",
     "iopub.status.idle": "2024-08-22T08:44:21.297508Z",
     "shell.execute_reply": "2024-08-22T08:44:21.296230Z"
    },
    "papermill": {
     "duration": 5.33202,
     "end_time": "2024-08-22T08:44:21.299132",
     "exception": true,
     "start_time": "2024-08-22T08:44:15.967112",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24/1112050501.py:5: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(filename)\n",
      "/opt/conda/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/librosa/core/audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/librosa/core/audio.py:209\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    657\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[0;32m--> 658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/soundfile.py:1216\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1215\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[0;32m-> 1216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "\u001b[0;31mLibsndfileError\u001b[0m: Error opening '': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Audio, display\n\u001b[1;32m      4\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# 파일 주소\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m y, sr \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m audio_wdt \u001b[38;5;241m=\u001b[39m Audio(data\u001b[38;5;241m=\u001b[39my,rate\u001b[38;5;241m=\u001b[39msr)\n\u001b[1;32m      7\u001b[0m display(audio_wdt)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/librosa/core/audio.py:184\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n\u001b[1;32m    181\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/librosa/util/decorators.py:59\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDeprecated as of librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mIt will be removed in librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[1;32m     58\u001b[0m )\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/librosa/core/audio.py:240\u001b[0m, in \u001b[0;36m__audioread_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    237\u001b[0m     reader \u001b[38;5;241m=\u001b[39m path\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43maudioread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[1;32m    243\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/audioread/__init__.py:127\u001b[0m, in \u001b[0;36maudio_open\u001b[0;34m(path, backends)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m BackendClass \u001b[38;5;129;01min\u001b[39;00m backends:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBackendClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError:\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/audioread/rawread.py:59\u001b[0m, in \u001b[0;36mRawAudioFile.__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m aifc\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "# (선택) Ipython 라이브러리를 이용해 학습에 사용될 음성을 직접 들어볼 수 있습니다.\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "filename = ''  # 파일 주소\n",
    "y, sr = librosa.load(filename)\n",
    "audio_wdt = Audio(data=y,rate=sr)\n",
    "display(audio_wdt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53efb269",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# **1. 데이터**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a3883a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## **(1) Label Map**\n",
    "{'강아지' : 0, '고양이' : 1} 등과 같은 형식으로, 머신러닝, 딥러닝 모델들은 feature나 label의 값들이 숫자(정수/실수)인 것만 처리할 수 있기 때문에, 문자열일 경우 숫자형으로 변환하여 처리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdd7b5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T12:57:08.842090Z",
     "iopub.status.busy": "2024-08-20T12:57:08.841015Z",
     "iopub.status.idle": "2024-08-20T12:57:08.848350Z",
     "shell.execute_reply": "2024-08-20T12:57:08.847109Z",
     "shell.execute_reply.started": "2024-08-20T12:57:08.842038Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_map = {'blues': 0, \n",
    "             'classical': 1, \n",
    "             'country': 2, \n",
    "             'disco': 3, \n",
    "             'hiphop': 4, \n",
    "             'jazz': 5, \n",
    "             'metal': 6, \n",
    "             'pop': 7, \n",
    "             'reggae': 8, \n",
    "             'rock': 9}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ca4e14",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## **(2) Feature Extract**\n",
    "컴퓨터에 입력할 수 있도록 샘플링과 양자화를 거쳤다고 해도, 이를 바로 분류기에 넣는 것은 높은 성능을 보장할 수 없습니다.<br>\n",
    "이는 음성 정보 안에 여러 주파수가 섞여 있으며 방대한 정보를 담고 있기 때문입니다.<br>\n",
    "따라서 데이터 중 음성의 대표적인 성질을 나타낼 수 있는 handcrafted feature를 추출하여 사용하는 것이 필수적입니다.<br>\n",
    "데이터에서 어떠한 특징을 어떠한 방식으로 추출하는지에 따라 분류기 성능에 큰 영향을 끼칠 수 있기 때문에 feature 추출을 위해서는 신중한 설계가 선행되어야 합니다.<br>\n",
    "\n",
    "- 참고 1 : https://librosa.org/doc/latest/index.html\n",
    "- 참고 2 : https://wikidocs.net/192879"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e37736",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### **Method 1. Rhythm**\n",
    "- Method 1에서는 rhythm 도메인의 두 가지 feature인 autocorrelation tempogram과 fourier tempogram을 추출합니다.\n",
    "- 단, librosa.load() 함수를 통해 얻은 time-series (y, sr)를 입력으로 주어 두 feature를 얻는 것을 목표로 합니다.\n",
    "- 각 단계별 세부 내용은 아래 함수를 참고하시기 바랍니다.\n",
    "\n",
    "(1-1) librosa.load(): extract_rhythm_features() 함수의 인자로 넘겨 받은 file_path에 대하여 1초 당 22050개의 샘플을 추출한 time-series를 load합니다.<br>\n",
    "(1-2) librosa.onset.onset_strength(): 앞서 load한 time-series를 함수의 입력으로 주어 onset_envelope를 추출합니다.<br>\n",
    "(1-3) librosa.feature.tempogram(): 앞서 추출한 onset_envelope와 sr을 함수의 입력으로 주어 autocorrelation tempogram feature를 추출합니다.<br>\n",
    "(1-4) autocorrelation tempogram에 대해 시간 축으로 평균을 내고 절대값을 취해 복소수를 제거함으로써 tempogram_feature를 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8dfd45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T13:00:48.702700Z",
     "iopub.status.busy": "2024-08-20T13:00:48.702320Z",
     "iopub.status.idle": "2024-08-20T13:00:48.709444Z",
     "shell.execute_reply": "2024-08-20T13:00:48.708466Z",
     "shell.execute_reply.started": "2024-08-20T13:00:48.702651Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_rhythm_features(file_path):\n",
    "    # 반환할 feature list 선언\n",
    "    feature = []\n",
    "    \n",
    "    # (1-1) librosa.load(): extract_rhythm_features() 함수의 인자로 넘겨 받은 file_path에 대하여 1초 당 22050개의 샘플을 추출한 time-series를 load합니다.\n",
    "    y, sr = librosa.load(file_path, sr=22050)\n",
    "    \n",
    "    # (1-2) librosa.onset.onset_strength(): 앞서 load한 time-series를 함수의 입력으로 주어 onset_envelope를 추출합니다.\n",
    "    onset_envelope = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "    \n",
    "    # autocorrelation tempogram\n",
    "    # (1-3) librosa.feature.tempogram(): 앞서 추출한 onset_envelope와 sr을 함수의 입력으로 주어 autocorrelation tempogram feature를 추출합니다. 함수의 입력 변수를 잘 확인하세요.\n",
    "    tempogram = librosa.feature.tempogram(onset_envelope=onset_envelope, sr=sr)\n",
    "    \n",
    "    # (1-4) autocorrelation tempogram에 대해 시간 축으로 평균을 내고 절대값을 취해 복소수를 제거함으로써 tempogram_feature를 얻습니다.\n",
    "    tempogram_feature = np.mean(np.abs(tempogram), axis=1)   # (384, 1293) -> (384, ), 복소수를 없애기 위해 절대값 처리\n",
    "    \n",
    "    feature = tempogram_feature\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2e8cb2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### **Method 2. Spectral**\n",
    "- Method 2에서는 spectral 도메인의 두 가지 feature인 MFCC와 chromagram을 추출합니다.\n",
    "- 단, librosa 라이브러리의 함수를 통해 spectrogram, power_spectrogram, melspectrogram, melspectrogram_db를 순차적으로 얻고, 이를 입력으로 주어 두 feature를 얻는 것을 목표로 합니다.\n",
    "- 각 단계별 세부 내용은 아래 함수를 참고하시기 바랍니다.\n",
    "\n",
    "(2-1) librosa.load(): extract_spectral_features() 함수의 인자로 넘겨 받은 file_path에 대하여 1초 당 22050개의 샘플을 추출한 time-series를 load합니다.<br>\n",
    "(2-2) librosa.stft(): 앞서 load한 time-series를 함수의 입력으로 주어 stft를 추출하고, 절대값을 취해 spectrogram을 추출합니다.<br>\n",
    "(2-3) 앞서 얻은 spectrogram에 제곱 연산을 취해 power_spectrogram을 얻습니다.<br>\n",
    "(2-4) librosa.feature.melspectrogram(): 앞서 얻은 power_spectrogram을 함수의 입력으로 주어 melspectrogram을 추출합니다.<br>\n",
    "(2-5) librosa.power_to_db(): 앞서 얻은 melspectrogram을 함수의 입력으로 주어 db scale로 변환된 melspectrogram_db를 추출합니다.<br>\n",
    "(2-6) librosa.feature.chroma_stft(): (2-3)에서 얻은 power_spectrogram을 함수의 입력으로 주어 chromagram을 추출합니다.<br>\n",
    "(2-7) chromagram에 대해 시간 축으로 평균을 내어 feature로 사용할 수 있도록 변환합니다.<br>\n",
    "(2-8) librosa.feature.mfcc(): (2-5)에서 얻은 melspectrogram_db를 함수의 입력으로 주어 mfcc를 추출합니다.<br>\n",
    "(2-9) mfcc에 대해 시간 축으로 평균을 내어 feature로 사용할 수 있도록 변환합니다.<br>\n",
    "(2-10) chromagram과 mfcc를 하나의 feature로 반환할 수 있도록 concatenate를 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a58fc7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T13:04:13.993986Z",
     "iopub.status.busy": "2024-08-20T13:04:13.993319Z",
     "iopub.status.idle": "2024-08-20T13:04:14.002623Z",
     "shell.execute_reply": "2024-08-20T13:04:14.001724Z",
     "shell.execute_reply.started": "2024-08-20T13:04:13.993951Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_spectral_features(file_path):\n",
    "    # 반환할 feature list 선언\n",
    "    feature = []\n",
    "    \n",
    "    # (2-1) librosa.load(): extract_spectral_features() 함수의 인자로 넘겨 받은 file_path에 대하여 1초 당 22050개의 샘플을 추출한 time-series를 load합니다.  \n",
    "    y, sr = librosa.load(file_path, sr=22050)\n",
    "\n",
    "    # (2-2) librosa.stft(): 앞서 load한 time-series를 함수의 입력으로 주어 stft를 추출하고, 절대값을 취해 복소수를 제거한 spectrogram을 추출합니다.  \n",
    "    # * 이 때, 93ms의 물리적 간격으로 나뉘어 spectrogram이 생성될 수 있도록 하이퍼파라미터 n_fft를 조절합니다. 어떠한 값을 넣어야 하는지는 공식 문서를 참고하세요.\n",
    "    n_fft = librosa.stft(y=y, n_fft=2048)\n",
    "    spectrogram = np.abs(n_fft)\n",
    "    \n",
    "    # (2-3) 앞서 얻은 spectrogram에 제곱 연산을 취해 power_spectrogram을 얻습니다.  \n",
    "    power_spectrogram = spectrogram ** 2\n",
    "    \n",
    "    # (2-4) librosa.feature.melspectrogram(): 앞서 얻은 power_spectrogram을 함수의 입력으로 주어 melspectrogram을 추출합니다.  \n",
    "    melspectrogram = librosa.feature.melspectrogram(S=power_spectrogram, sr=sr)\n",
    "    \n",
    "    # (2-5) librosa.power_to_db(): 앞서 얻은 melspectrogram을 함수의 입력으로 주어 db scale로 변환된 melspectrogram_db를 추출합니다.  \n",
    "    melspectrogram_db = librosa.power_to_db(melspectrogram)\n",
    "    \n",
    "    # chromagram\n",
    "    # (2-6) librosa.feature.chroma_stft(): (2-3)에서 얻은 power_spectrogram을 함수의 입력으로 주어 chromagram을 추출합니다. 함수의 입력 변수를 잘 확인하세요.\n",
    "    chromagram = librosa.feature.chroma_stft(S=power_spectrogram, sr=sr)  # (12, 1293)\n",
    "    \n",
    "    # (2-7) chromagram에 대해 시간 축으로 평균을 내어 chromagram_feature를 얻습니다. \n",
    "    chromagram_feature = np.mean(chromagram, axis=1)  # (12, )\n",
    "    \n",
    "    # mfcc\n",
    "    # (2-8) librosa.feature.mfcc(): (2-5)에서 얻은 melspectrogram_db를 함수의 입력으로 주어 mfcc를 추출합니다. 함수의 입력 변수를 잘 확인하세요.\n",
    "    mfcc = librosa.feature.mfcc(S=melspectrogram_db, sr=sr)  # (20, 1293)\n",
    "    \n",
    "    # (2-9) mfcc에 대해 시간 축으로 평균을 내어 mfcc_feature를 얻습니다.  \n",
    "    mfcc_feature = np.mean(mfcc, axis=1)  # (20, )\n",
    "    \n",
    "    # (2-10) chromagram과 mfcc를 하나의 feature로 반환할 수 있도록 concatenate를 수행합니다.\n",
    "    feature = np.concatenate((chromagram_feature, mfcc_feature))\n",
    "\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ff8896",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## **(3) Custom Dataset**\n",
    "Custom Dataset 클래스는 크게 **__init__(), __len__(), 그리고 __getitem__()** 3개의 함수로 구현해야 합니다. \n",
    "\n",
    "1.  __init__()\n",
    "    - Dataset instance를 생성할 때 한번만 실행되는 함수로, 입력 이미지의 디렉토리와 라벨 정보 그리고 transform을 초기화 합니다. \n",
    "\n",
    "2. __len__()\n",
    "    - 데이터셋의 샘플 개수를 반환하는 함수 입니다. \n",
    "    \n",
    "3. __getitem__()\n",
    "    - 주어진 인덱스에 해당하는 데이터 샘플을 데이터셋에서 불러오고 반환하는 함수 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95899a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T13:06:53.094351Z",
     "iopub.status.busy": "2024-08-20T13:06:53.093863Z",
     "iopub.status.idle": "2024-08-20T13:06:53.105875Z",
     "shell.execute_reply": "2024-08-20T13:06:53.104567Z",
     "shell.execute_reply.started": "2024-08-20T13:06:53.094321Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_path, split, extract_features, label_map):\n",
    "        self.split = split.upper()\n",
    "        self.root_path = root_path\n",
    "        self.extract_features = extract_features\n",
    "        self.label_map = label_map\n",
    "        \n",
    "        self.music = []\n",
    "        self.label = []\n",
    "        \n",
    "        # (3-1) test 데이터 디렉토리 경로 초기화\n",
    "        if self.split == 'TEST':\n",
    "            musics = sorted(os.listdir(self.root_path))\n",
    "            for music_ in musics:\n",
    "                \n",
    "            \n",
    "        # (3-2) train 데이터 디렉토리 경로, 라벨 초기화\n",
    "        else:\n",
    "            genres = sorted(os.listdir(self.root_path))\n",
    "            for genre_ in genres:\n",
    "                genre_path = os.path.join(self.root_path, genre_)\n",
    "                if not os.path.isdir(genre_path):\n",
    "                    continue\n",
    "                musics = sorted(os.listdir(genre_path))\n",
    "                for music_ in musics:\n",
    "                    \n",
    "                    \n",
    "            \n",
    "    # 전체 데이터 샘플 개수 반환\n",
    "    def __len__(self):\n",
    "        return len(self.music)\n",
    "    \n",
    "    # 주어진 인덱스에 해당하는 데이터 반환\n",
    "    def __getitem__(self, idx):        \n",
    "        # (3-3) 음악 데이터에 feature extract 적용\n",
    "        if self.extract_features == 'spectral':\n",
    "            music_feature = extract_spectral_features(self.music[idx])\n",
    "        elif self.extract_features == 'rhythm':\n",
    "            music_feature = extract_rhythm_features(self.music[idx])\n",
    "        \n",
    "        # (3-4) test에 사용할 데이터 반환\n",
    "        if self.split == 'TEST':\n",
    "            return \n",
    "        # (3-5) train에 사용할 데이터 반환\n",
    "        else:\n",
    "            label = self.label_map[self.label[idx]]\n",
    "            return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec78844",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### **(4) 데이터셋과 데이터로더**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc1d81b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (4-1) 훈련 및 테스트 데이터셋 로드\n",
    "train_dataset = CustomDataset(root_path=args['train_path'], split='TRAIN', extract_features=args[\"extract_features\"],label_map=label_map)\n",
    "test_dataset = CustomDataset(root_path=args['test_path'], split='TEST', extract_features=args[\"extract_features\"], label_map=label_map) \n",
    "\n",
    "# (4-2) 데이터로더 정의\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c975fd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Train Music Data\\'s shape : {train_dataset.__getitem__(0)[0].shape}')   # train_dataset의 반환값 : music_feature, label\n",
    "print(f'Test Music Data\\'s shape : {test_dataset.__getitem__(0).shape}')        # test_dataset의 반환값 : music_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2445fdb3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# **3. 모델**\n",
    "모델을 직접 설계하여 프로젝트를 수행하세요.\n",
    "\n",
    "직접 쌓기만 하신다면, 어떤 모델이든 사용 가능합니다.\n",
    "\n",
    "아래는 베이스라인 모델입니다.\n",
    "\n",
    "> ```\n",
    ">         Layer (type)               Output Shape         Param #\n",
    "> =================================================================\n",
    ">             Conv1d-1              [-1, 64, 384]             256\n",
    ">               ReLU-2              [-1, 64, 384]               0\n",
    ">             Conv1d-3              [-1, 64, 384]          12,352\n",
    ">               ReLU-4              [-1, 64, 384]               0\n",
    ">          MaxPool1d-5              [-1, 64, 192]               0\n",
    ">             Conv1d-6             [-1, 128, 192]          24,704\n",
    ">               ReLU-7             [-1, 128, 192]               0\n",
    ">             Conv1d-8             [-1, 128, 192]          49,280\n",
    ">               ReLU-9             [-1, 128, 192]               0\n",
    ">         MaxPool1d-10              [-1, 128, 96]               0\n",
    ">            Conv1d-11              [-1, 256, 96]          98,560\n",
    ">              ReLU-12              [-1, 256, 96]               0\n",
    ">            Conv1d-13              [-1, 256, 96]         196,864\n",
    ">              ReLU-14              [-1, 256, 96]               0\n",
    ">            Conv1d-15              [-1, 256, 96]         196,864\n",
    ">              ReLU-16              [-1, 256, 96]               0\n",
    ">         MaxPool1d-17              [-1, 256, 48]               0\n",
    ">            Conv1d-18              [-1, 512, 48]         393,728\n",
    ">              ReLU-19              [-1, 512, 48]               0\n",
    ">            Conv1d-20              [-1, 512, 48]         786,944\n",
    ">              ReLU-21              [-1, 512, 48]               0\n",
    ">            Conv1d-22              [-1, 512, 48]         786,944\n",
    ">              ReLU-23              [-1, 512, 48]               0\n",
    ">         MaxPool1d-24              [-1, 512, 24]               0\n",
    ">            Conv1d-25              [-1, 512, 24]         786,944\n",
    ">              ReLU-26              [-1, 512, 24]               0\n",
    ">            Conv1d-27              [-1, 512, 24]         786,944\n",
    ">              ReLU-28              [-1, 512, 24]               0\n",
    ">            Conv1d-29              [-1, 512, 24]         786,944\n",
    ">              ReLU-30              [-1, 512, 24]               0\n",
    ">         MaxPool1d-31              [-1, 512, 12]               0\n",
    "> AdaptiveAvgPool1d-32               [-1, 512, 1]               0\n",
    ">           Flatten-33                  [-1, 512]               0\n",
    ">            Linear-34                   [-1, 10]           5,130\n",
    "> =================================================================\n",
    "> Total params: 4,912,458\n",
    "> Trainable params: 4,912,458\n",
    "> Non-trainable params: 0\n",
    "> ----------------------------------------------------------------\n",
    "> Input size (MB): 0.00\n",
    "> Forward/backward pass size (MB): 4.74\n",
    "> Params size (MB): 18.74\n",
    "> Estimated Total Size (MB): 23.48\n",
    "> ----------------------------------------------------------------\n",
    "> ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc7bfbd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (5) 모델 설계\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        모델 초기화 함수입니다.\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernal_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernal_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=256, out_channels=512, kernal_size=3, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=512, out_channels=1024, kernal_size=3, padding=1),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(output_size=1))\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=1024, out_features=10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        모델의 순전파 함수입니다.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): 입력 데이터\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: 출력 예측값\n",
    "        \"\"\"\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = x.permute(0,2,1)\n",
    "        \n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.conv_block4(x)\n",
    "        x = self.conv_block5(x)\n",
    "        \n",
    "        x= self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e071d613",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 모델을 장치로 이동\n",
    "model = Model().to(device)\n",
    "\n",
    "# 모델 요약 정보 출력\n",
    "summary_model(model, input_shape=train_dataset.__getitem__(0)[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b758761",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# **4. 학습**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83457b09",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(train_dataloader, model, device, args):\n",
    "    \"\"\"\n",
    "    모델을 훈련시키는 함수입니다.\n",
    "\n",
    "    Args:\n",
    "        train_dataloader (DataLoader): 훈련 데이터가 포함된 데이터로더\n",
    "        model (nn.Module): 훈련할 신경망 모델\n",
    "        device (torch.device): 모델과 데이터를 올릴 장치 (CPU 또는 GPU)\n",
    "        args (dict): 훈련 설정을 포함한 딕셔너리 (예: 에포크 수)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # (6-1) 옵티마이저와 손실 함수 정의\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args[\"lr\"], eps=args[\"eps\"])\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # 모델의 모든 파라미터의 기울기를 0으로 초기화\n",
    "    model.zero_grad()\n",
    "\n",
    "    # 지정된 에포크 수만큼 훈련 반복\n",
    "    for epoch in range(args[\"epochs\"]):\n",
    "        print(f'Epoch {epoch + 1}/{args[\"epochs\"]}')\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        \n",
    "        # 훈련 데이터로더에서 데이터를 반복적으로 가져옴\n",
    "        for data, label in tqdm(train_dataloader):\n",
    "            # 데이터를 부동 소수점 형식으로 변환하고 장치에 올림\n",
    "            data = data.float().to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            # (6-2) 모델을 사용하여 예측 수행\n",
    "            pred = model(data)\n",
    "            \n",
    "            # (6-3) 예측값과 실제 라벨을 사용하여 손실 계산\n",
    "            loss = loss_fn(pred, label)\n",
    "            \n",
    "            # (6-4) 옵티마이저의 기울기를 초기화\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # (6-5) 역전파를 통해 기울기 계산\n",
    "            loss.backward()\n",
    "            \n",
    "            # (6-6) 옵티마이저를 사용하여 모델의 파라미터 업데이트\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 총 손실 계산\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # 예측값에서 가장 높은 값의 인덱스를 선택\n",
    "            pred = pred.argmax(dim=1)\n",
    "            # 정확도 계산\n",
    "            train_acc += (pred == label).sum().item()\n",
    "        \n",
    "        # 평균 손실과 정확도 계산\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_acc /= len(train_dataloader.dataset)\n",
    "            \n",
    "        # 모델과 옵티마이저 상태를 저장\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model': model,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss.item,\n",
    "            }, f'./results/model_state_dict_epoch_{epoch+1}.pth')\n",
    "            \n",
    "        # 현재 에포크의 손실과 정확도 출력\n",
    "        print(f'CheckPoint : model_state_dict_epoch_{epoch+1}.pth')\n",
    "        print(f'Train Loss : {train_loss}, Train Accuracy : {train_acc}\\n')\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 훈련 함수 호출\n",
    "    train(train_dataloader, model, device, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e7307b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# **5. 평가**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10b872d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(test_dataloader, model, device):\n",
    "    \"\"\"\n",
    "    모델의 예측을 수행하는 함수입니다.\n",
    "\n",
    "    Args:\n",
    "        test_dataloader (DataLoader): 테스트 데이터가 포함된 데이터로더\n",
    "        model (nn.Module): 예측에 사용할 신경망 모델\n",
    "        device (torch.device): 모델과 데이터를 올릴 장치 (CPU 또는 GPU)\n",
    "\n",
    "    Returns:\n",
    "        preds (list): 예측된 클래스의 정수 인덱스 리스트\n",
    "    \"\"\"\n",
    "    # 모델을 평가 모드로 전환\n",
    "    model.eval()\n",
    "    # 예측 결과를 저장할 리스트 초기화\n",
    "    preds = []\n",
    "    \n",
    "    # 데이터로더에서 데이터를 반복적으로 가져옴\n",
    "    for data in tqdm(test_dataloader):\n",
    "        # 데이터를 부동 소수점 형식으로 변환하고 장치에 올림\n",
    "        data = data.float().to(device)\n",
    "        \n",
    "        # 기울기 계산을 비활성화하여 예측 수행\n",
    "        with torch.no_grad():\n",
    "            # (7-1) 모델을 사용하여 예측 수행\n",
    "            pred = model(data)\n",
    "            \n",
    "            # (7-2) argmax를 이용하여 예측 결과에서 가장 높은 값의 인덱스를 선택\n",
    "            pred = pred.argmax(dim=1).cpu().numpy()\n",
    "            \n",
    "            # (7-3) 예측 결과를 리스트에 추가\n",
    "            preds.extend(pred)\n",
    "            \n",
    "    return preds\n",
    "            \n",
    "\n",
    "def int_to_label(label_map, predictions_int):\n",
    "    \"\"\"\n",
    "    정수 형태의 예측값 리스트를 클래스 라벨로 변환하는 함수입니다.\n",
    "\n",
    "    Args:\n",
    "        label_map (dict): 클래스 라벨과 정수 인덱스 매핑이 저장된 딕셔너리\n",
    "        predictions_int (list): 정수 형태의 예측값 리스트\n",
    "\n",
    "    Returns:\n",
    "        predicted_labels (list): 문자열 형태의 예측 클래스 라벨 리스트\n",
    "    \"\"\"\n",
    "    # 정수 예측값을 클래스 라벨로 변환하여 리스트에 저장\n",
    "    predicted_labels = [list(label_map.keys())[list(label_map.values()).index(pred)] for pred in predictions_int]\n",
    "    \n",
    "    return predicted_labels\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 예측값을 얻기 위해 test 함수 호출\n",
    "    preds = test(test_dataloader, model, device)\n",
    "    \n",
    "    # 정수 예측값을 클래스 라벨로 변환\n",
    "    preds = int_to_label(label_map, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34aadcd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv(args[\"submit_path\"])\n",
    "submit['genre'] = preds\n",
    "submit.to_csv('submission_p3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0521f059",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submit"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9018673,
     "sourceId": 82520,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 29.946123,
   "end_time": "2024-08-22T08:44:23.130046",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-22T08:43:53.183923",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
